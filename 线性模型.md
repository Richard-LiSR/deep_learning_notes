## 线性模型

一般来说，线性模型主要适用于在训练过程中

假设令训练结果y_hat = x ＊ｗ（简化模型）

所以机器学习在训练过程中首先需要做一个随机猜测，而ｗ就是随机值

目的是为了找到评估模型的误差，使得最上面那条线，无限接近真实值，因此我们就需要创建一个评估模型

![image-20210704094658445](http://richard-lsr.top/image-20210704094658445.png)

而为了更好的评估模型需要在机器学习中构建Loss(损失函数)

![image-20210704095140519](http://richard-lsr.top/image-20210704095140519.png)

需要注意的是损失函数是针对一个样本的，而对于整个训练集，需要求其平均平方误差即均方误差（MSE）

![image-20210704095514012](http://richard-lsr.top/image-20210704095514012.png)

### 函数表示

````python
import numpy as np
import matplotlib.pyplot as plt
# 穷举发确定权重范围，然后通过线性模型确定确定最优值
x_data = [1.0, 2.0, 3.0]
y_data = [2.0, 4.0, 6.0]

def forward(x):
    return x * w

def loss(x, y):
    y_pred = forward(x)
    return (y_pred - y) * (y_pred - y)
# 定义损失函数： (y_pred - y )^2 = (x * w -y )^2

w_list = []
mse_list = []
for w in np.arange(0.0, 4.1, 0.1):
    print('w=', w)
    l_sum = 0
    for x_val, y_val in zip(x_data, y_data):
        y_pred_val = forward(x_val)
        loss_val = loss(x_val, y_val)
        l_sum += loss_val
        print('\t', x_val, y_val, y_pred_val, loss_val)
    print('MSE=', l_sum / 3)
    w_list.append(w)
    mse_list.append(l_sum / 3)
plt.plot(w_list, mse_list)
plt.ylabel('loss')
plt.xlabel('w')
plt.show()
````

#### 结果

![image-20210704100017300](http://richard-lsr.top/image-20210704100017300.png)

## 梯度下降

梯度下降算法是为了更好寻找到最优值（即让损失函数达到最小）

![image-20210704101240501](http://richard-lsr.top/image-20210704101240501.png)

在梯度下降算法中为了更好的找到最优权重，通常对权重值进行迭代更新，常用公式如下：

梯度：

***曲面上方向导数的最大值的方向就代表了梯度的方向，因此我们在做梯度下降的时候，应该是沿着梯度的反方向进行权重的更新，可以有效的找到全局的最优解。***

![image-20210704104228012](http://richard-lsr.top/image-20210704104228012.png)

***权重更新表达式为：*****w = w - a * 梯度**

（其中a代表学习率，相当于权重更新的步长，而学习率通常取值小【0.01左右】）

![image-20210704103258216](http://richard-lsr.top/image-20210704103258216.png)

**注意：梯度下降算法只能找到局部最优点（尤其在非凸函数中）**

 而在整个训练过程中，其梯度为: (以上节线性模型为例)

![image-20210704104546458](http://richard-lsr.top/image-20210704104546458.png)

- **最后更新权重式子为**：

![](http://richard-lsr.top/20210704104853.png)

### 代码实现

```python
import numpy as np
import matplotlib.pyplot as plt

x_data = [1.0, 2.0, 3.0]
y_data = [2.0, 4.0, 6.0]

w = 1.0

def forward(x):
    return x * w

def cost(x_set, y_set):
    cost = 0
    for x, y in zip(x_set, y_set):
        y_pred = forward(x)
        cost += (y_pred - y) ** 2
    return cost / len(x_set)

def gradient(x_set, y_set):
    grad = 0
    for x, y in zip(x_set, y_set):
        grad += 2 * x * (x * w - y)
    return grad / len(x_set)
#     得出梯度函数的解析式

print("predict (before training)", 4, forward(4))
cost_list = []
epoch_list = []
for epoch in range(100):
    cost_val = cost(x_data, y_data)
    grad_val = gradient(x_data, y_data)
    w -= 0.01 * grad_val
    w = round(w, 2)
    cost_val = round(cost_val, 2)
    # 将w和cost_val 保留两位小数
    print("epoch:", epoch, 'w=', w, "loss=", cost_val)
    epoch_list.append(epoch)
    cost_list.append(cost_val)
print("predict (after training )", 4, forward(4))
plt.plot(epoch_list, cost_list)
plt.xlabel("Epoch")
plt.ylabel("cost")
plt.show()

```

#### 结果

![image-20210704105356160](http://richard-lsr.top/image-20210704105356160.png)

![](http://richard-lsr.top/20210704105533.png)

#### 结论

随着更新迭代次数的增加，cost损失函数逐渐趋近与0，而权重逐渐稳定于2

**注意：在平常训练集时，损失是不可能达到0的，而平常训练时，损失值会呈现出锯齿状下降，所以为了更好的用图表显示，所以对损失函数进行一个指数加权平均，来使图片变得平滑**。

## 随机梯度下降

随机梯度下降是梯度下降的衍生版，**是数据集中随机选出一个数据来进行损失计算**，具备性能高，速度快的特点

### 具体的公式区别

![image-20210704143734161](http://richard-lsr.top/image-20210704143734161.png)

### 代码实现

```python
import numpy as np
import matplotlib.pyplot as plt

x_data = [1.0, 2.0, 3.0]
y_data = [2.0, 4.0, 6.0]

w = 1.0

def forward(x):
    return x * w

def loss(x_set, y_set):
    y_pred = forward(x)
    return (y_pred - y) ** 2

def gradient(x_set, y_set):
    return 2 * x * (x * w - y)

print("predict (before training)", 4, forward(4))

cost_list = []
epoch_list = []
for epoch in range(100):
   	for x,y in zip(x_data,y_data):
        grad = gradient(x,y)
        w = w- 0.01 * grad
        print("\tgrad:",x,y,grad)
        l = loss(x,y)
   	print("progress:",epoch,"w=", w,"loss=",l)
       
print("predict (after training )", 4, forward(4))
plt.plot(epoch_list, cost_list)
plt.xlabel("Epoch")
plt.ylabel("cost")
plt.show()

```

